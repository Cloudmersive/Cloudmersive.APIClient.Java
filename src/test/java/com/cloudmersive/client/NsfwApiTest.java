/*
 * imageapi
 * Image Recognition and Processing APIs let you use Machine Learning to recognize and process images, and also perform useful image modification operations.
 *
 * OpenAPI spec version: v1
 * 
 *
 * NOTE: This class is auto generated by the swagger code generator program.
 * https://github.com/swagger-api/swagger-codegen.git
 * Do not edit the class manually.
 */


package com.cloudmersive.client;

import java.io.File;
import com.cloudmersive.client.model.NsfwResult;
import org.junit.Test;
import org.junit.Ignore;


import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * API tests for NsfwApi
 */
@Ignore
public class NsfwApiTest {

    private final NsfwApi api = new NsfwApi();

    
    /**
     * Not safe for work (NSFW) content classification for Images
     *
     * Classify an image into Not Safe For Work (NSFW)/Pornographic/Nudity/Racy content and Safe Content.  Helpful for filtering out unsafe content when processing user images.  Input image should be JPG, PNG or GIF.  Consumes 20 API calls.
     *
     * @throws Exception
     *          if the Api call fails
     */
    @Test
    public void nsfwClassifyTest() throws Exception {
        File imageFile = null;
        NsfwResult response = api.nsfwClassify(imageFile);

        // TODO: test validations
    }
    
    /**
     * Not safe for work (NSFW) content classification for Documents
     *
     * Classify a document (PDF, DOCX, DOC, XLSX, XLS, PPTX, PPT) into Not Safe For Work (NSFW)/Pornographic/Nudity/Racy content and Safe Content.  Helpful for filtering out unsafe content when processing user images.  Consumes 20 API calls per image.
     *
     * @throws Exception
     *          if the Api call fails
     */
    @Test
    public void nsfwClassifyDocumentTest() throws Exception {
        File imageFile = null;
        NsfwResult response = api.nsfwClassifyDocument(imageFile);

        // TODO: test validations
    }
    
    /**
     * Not safe for work (NSFW) content classification for Video
     *
     * Classify a video into Not Safe For Work (NSFW)/Pornographic/Nudity/Racy content and Safe Content.  Helpful for filtering out unsafe content when processing user images.  Input image should be MP4, MOV, WEBM, MKV, AVI, FLV, MPG, GIF.  Consumes 20 API calls per frame analyzed.  Requires Cloudmersive Managed Instance or Private Cloud deployment.
     *
     * @throws Exception
     *          if the Api call fails
     */
    @Test
    public void nsfwClassifyVideoTest() throws Exception {
        File videoFile = null;
        NsfwResult response = api.nsfwClassifyVideo(videoFile);

        // TODO: test validations
    }
    
}
